# Welcome to my project

## Project info

**URL**: (https://usesayit.vercel.app/)
ABOUT PROJECT
SayIt â€” Multilingual, Hands-Free AAC Communication App

SayIt is an AI-powered Augmentative and Alternative Communication (AAC) web application designed to help people with speech and motor impairments communicate naturally, expressively, and independently.

The app combines multilingual text-to-speech, voice and tone customization, and hands-free head & facial gesture controls to make communication accessible to users who cannot easily speak or use their hands.

ğŸŒ Why SayIt Matters

Many AAC tools are limited by:

Few supported languages

Lack of personalization

Heavy reliance on touch input

SayIt removes these barriers by enabling:

Communication in multiple languages (including Nigerian languages)

Personalized voices and tones

Head-movement and mouth-gesture navigation for hands-free use

This makes SayIt especially useful for people with conditions such as ALS, cerebral palsy, autism, stroke recovery, or temporary speech limitations.

âœ¨ Key Features
ğŸ§  Pre-Built Phrase Library

Quickly communicate using commonly used phrases and expressions

Reduces the need for typing from scratch

ğŸŒ Multilingual Support

Speak in any language, including Nigerian and other global languages

Seamless switching between languages

ğŸ”Š Voice & Tone Customization

Change voice type

Select tone (calm, casual, polite, expressive, etc.)

Allows users to express how they feel, not just what they say

ğŸ¤– Hands-Free Head & Facial Controls

Move head up/down to navigate or scroll

Turn head left/right to switch selections

Open and close mouth to click or select buttons

This allows full interaction without touching the screen.

â™¿ Accessibility First

SayIt is built with accessibility at its core:

Hands-free navigation for users with limited motor control

Audio-based communication for non-verbal users

Language inclusivity for underserved communities

ğŸ› ï¸ Tech Stack

This project is built using modern web technologies:

React

TypeScript

Vite

Tailwind CSS

shadcn/ui

ğŸš€ Getting Started (Local Development)
Prerequisites

Node.js (v18+ recommended)

npm

Installation
# Clone the repository
git clone https://github.com/Mimi77499/head-click-magic.git

# Navigate into the project directory
cd head-click-magic

# Install dependencies
npm install

# Start the development server
npm run dev


The app will be available at:

http://localhost:5173

ğŸŒ Deployment

This app can be deployed on platforms such as Vercel, Netlify, or any modern frontend hosting service.

Example (Vercel)

Push your code to GitHub

Import the repository into Vercel

Deploy with default settings

ğŸ§ª Usage Notes

A camera is required for head and facial gesture detection

Best used in well-lit environments

Works on desktop and supported mobile browsers

ğŸ“Œ Use Cases

AAC communication for non-verbal users

Hands-free computer interaction

Assistive technology for motor impairments

Multilingual communication support

Accessibility-focused education tools

ğŸ›£ï¸ Future Improvements

Custom phrase creation by users

Saved user profiles (voice, language, tone)

Offline mode

Expanded gesture customization

Caregiver / assistant mode

ğŸ¤ Contributing

Contributions are welcome!

Fork the repository

Create a feature branch

Commit your changes

Open a pull request

ğŸ“„ License

This project is open-source and available under the MIT License.


